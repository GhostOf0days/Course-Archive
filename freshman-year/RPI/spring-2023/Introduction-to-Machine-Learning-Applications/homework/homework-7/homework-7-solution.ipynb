{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946072c5",
   "metadata": {
    "id": "946072c5",
    "nbgrader": {
     "grade": false,
     "grade_id": "Instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Homework-7\n",
    "\n",
    "\n",
    "##### Total number of points: 40\n",
    "#### Due date: April 11th, 2023\n",
    "\n",
    "Before you submit this homework, make sure everything runs as expected. First, restart the kernel (in the menu, select Kernel → Restart) and then run all cells (in the menubar, select Cell → Run All). You can discuss with others regarding the homework but all work must be your own.\n",
    "\n",
    "This homework will test your knowledge on random forests (including feature importances), and neural networks. The Python notebooks shared will be helpful to solve these problems.  \n",
    "\n",
    "Steps to evaluate your solutions:\n",
    "\n",
    "Step-1: Try on Colab or Anaconda (Windows: https://docs.anaconda.com/anaconda/install/windows/ ; Mac:https://docs.anaconda.com/anaconda/install/mac-os/ ; Linux: https://docs.anaconda.com/anaconda/install/linux/)\n",
    "\n",
    "Step-2: Open the Jupyter Notebook by first launching the anaconda software console\n",
    "\n",
    "Step-3: Open the homework's .ipynb file and write your solutions at the appropriate location \"# YOUR CODE HERE\"\n",
    "\n",
    "Step-4: You can restart the kernel and click run all (in the menubar, select Cell → Run All) on the center-right on the top of this window.\n",
    "\n",
    "Step-5: Now go to \"File\" then click on \"Download as\" then click on \"Notebook (.ipynb)\" Please DO NOT change the file name and just keep it as \".ipynb\"\n",
    "\n",
    "Step-6: Go to lms.rpi.edu and upload your homework at the appropriate link to submit this homework.\n",
    "\n",
    "\n",
    "#### Please note that for any question in this assignment you will receive points ONLY if your solution passes all the test cases including hidden testcases as well. So please make sure you try to think all possible scenarios before submitting your answers.  \n",
    "- Note that hidden tests are present to ensure you are not hardcoding. \n",
    "- If caught cheating: \n",
    "    - you will receive a score of 0 for the 1st violation. \n",
    "    - for repeated incidents, you will receive an automatic 'F' grade and will be reported to the dean of Lally School of Management. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f8f6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef6f8f6a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-92f5478d5bf05f89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "586328f4-c81f-48cd-f2bb-3e3eb469896d"
   },
   "outputs": [],
   "source": [
    "#Please run this cell to load the dataset. \n",
    "url = 'https://raw.githubusercontent.com/lmanikon/lmanikon.github.io/master/teaching/datasets/dataset_Facebook.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "df  = pd.read_csv(url)\n",
    "df = df.dropna()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f1f1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "287f1f1e",
    "outputId": "625199f4-e18e-4542-f320-ee49a84b8f93"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c66d4",
   "metadata": {
    "id": "340c66d4",
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-desc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Part-1 [24 points]: We will leverage randomforest classifier to identify the best features that predict 'Total Interactions'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87435723",
   "metadata": {
    "id": "87435723",
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-question",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1. Create a new dataframe `df_sub2` that contains only these feature columns in `df` : \n",
    "- `Page total likes`, `Type`, `Category`, `Post Month`, `Post Weekday`, `Post Hour`, `Paid`,  `Total Interactions`\n",
    "\n",
    "#### 2. Transform the categorical attribute `Type` in `df_sub2` to numerical attribute this way: \n",
    "- `Link`:1, `Photo`:2, `Status`:3, `Video`:4\n",
    "\n",
    "#### 3. Perform Standardization (using this formula https://en.wikipedia.org/wiki/Standard_score) only on `Page total likes` column in `df_sub2` \n",
    "- Please use `<dataframe>['<column>'].mean()` `<dataframe>['<column>'].std()` if you are using the mean and std values to manipulate the column. \n",
    "    \n",
    "#### 4. Using `df_sub2` perform train_test_split operation to build training (`X_train`, `y_train`) and testing data (`X_test`, `y_test`).\n",
    "- Use test_size=0.3, random_state=42 as the parameters for train_test_split() function. \n",
    "- Feature columns (X): `Page total likes`, `Type`, `Category`, `Post Month`, `Post Weekday`, `Post Hour`, `Paid`\n",
    "- CLass label column (y): `Total Interactions`\n",
    "\n",
    "#### 5. Train the randomforest classifier (initialized as variable `rf`) using these parameters: max_depth=3, random_state=0. \n",
    "- Using the trained model `rf` compute the impurity-based feature importances.\n",
    "- Append the names of these top-3 features to list `impFeatures`. Please make sure you type the feature names exactly as in df_sub2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cb344",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f5cb344",
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-sol",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "d62a4708-a650-4857-a4dc-ef52569c192c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df_sub2=df.loc[:, ['Page total likes', 'Type', 'Category', 'Post Month', 'Post Weekday', 'Post Hour', 'Paid', 'Total Interactions']]\n",
    "print(set(df_sub2.columns))\n",
    "print(len(df_sub2))\n",
    "print(len(df_sub2.iloc[0,:]))\n",
    "\n",
    "df_sub2['Type']=df_sub2['Type'].map({'Link':1, 'Photo':2, 'Status':3, 'Video':4})\n",
    "meanval=df_sub2['Page total likes'].mean()\n",
    "stdval=df_sub2['Page total likes'].std()\n",
    "df_sub2['Page total likes']=(df_sub2['Page total likes']-meanval)/stdval\n",
    "\n",
    "\n",
    "\n",
    "X=df_sub2.drop(columns=['Total Interactions'])\n",
    "y=df_sub2['Total Interactions']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(len(X_train))\n",
    "print(len(y_test))\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "feats=(rf.feature_importances_)\n",
    "colnames=list(X.columns)\n",
    "\n",
    "diction = dict(zip(colnames, feats))\n",
    "\n",
    "import operator\n",
    "sorted_x = sorted(diction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "impFeatures=[]\n",
    "impFeatures.append(sorted_x[0][0])\n",
    "impFeatures.append(sorted_x[1][0])\n",
    "impFeatures.append(sorted_x[2][0])\n",
    "print(impFeatures)\n",
    "\n",
    "print(rf.n_classes_)\n",
    "print(rf.n_features_)\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa279b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74fa279b",
    "outputId": "8756e89e-a5aa-485e-b5da-1e3d4964da60"
   },
   "outputs": [],
   "source": [
    "diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1042a",
   "metadata": {
    "id": "0ae1042a",
    "nbgrader": {
     "grade": true,
     "grade_id": "q11",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Cell-1 -- Do not modify this cell\n",
    "assert set(df_sub2.columns)=={'Post Hour', 'Total Interactions', 'Post Weekday', 'Category', 'Paid', 'Post Month', 'Page total likes', 'Type'}\n",
    "assert len(df_sub2)==495\n",
    "assert len(df_sub2.iloc[0,:])==8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e25ec",
   "metadata": {
    "id": "5a9e25ec",
    "nbgrader": {
     "grade": true,
     "grade_id": "q12",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Cell-2 -- Do not modify this cell\n",
    "assert round(df_sub2['Page total likes'].mean(),0)==0\n",
    "assert len(X_train)==346\n",
    "assert 'Post Hour' in impFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd8fe4",
   "metadata": {
    "id": "8bbd8fe4",
    "nbgrader": {
     "grade": true,
     "grade_id": "q13",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Cell-3 -- Do not modify this cell\n",
    "assert rf.n_classes_==230\n",
    "assert len(impFeatures)==3\n",
    "assert math.ceil(df_sub2['Page total likes'].std())==1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53163b00",
   "metadata": {
    "id": "53163b00",
    "nbgrader": {
     "grade": true,
     "grade_id": "q14",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Cell-4 -- Do not modify this cell\n",
    "assert (rf.n_features_in_)==7\n",
    "assert len(y_test)==149\n",
    "assert set(impFeatures)=={'Page total likes', 'Post Hour', 'Post Weekday'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba946a55",
   "metadata": {
    "id": "ba946a55",
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-desc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Part-2 [16 points]: We will now use neural networks to model a regression problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ec561",
   "metadata": {
    "id": "7e8ec561",
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-question",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1. Create a new dataframe `df_sub4` that contains only these feature columns in `df` : \n",
    "- `Page total likes`, `Type`, `Category`, `Post Month`, `Post Weekday`, `Post Hour`, `Paid`,  `Total Interactions`\n",
    "\n",
    "#### 2. Perform one-hot encoding on these features below in the dataframe `df_sub4` to create a new dataframe `df_OHE`\n",
    "- `Type`, `Category`, `Post Month`, `Post Weekday`, `Post Hour`\n",
    "\n",
    "#### 3. Perform normalization only on `Page total likes` and `Total Interactions` column in `df_OHE` using MinMaxScaler or minmax_scale \n",
    "- note that both these functions will output the same result\n",
    "\n",
    "#### 4. Using `df_OHE` perform train_test_split operation to build training (`X_train`, `y_train`) and testing data (`X_test`, `y_test`).\n",
    "- Use test_size=0.3, random_state=42 as the parameters for train_test_split() function. \n",
    "- Feature columns (X): Everything except `Total Interactions`\n",
    "- CLass label column (y): `Total Interactions`\n",
    "\n",
    "#### 5. Build a 3-layer neural network with `relu` as the 2 hidden layers' activation function and `sigmoid` as the final layer's activation function. \n",
    "- Use the same `adam` optimizer and `mean_squared_error` as the loss function. \n",
    "- `epochs`=150, `batch_size`=10\n",
    "- since the class label is continuous, we use `metrics.MeanSquaredError()` as the metric (`from keras import metrics`) to measure the performance of our classifier captured as variable `mse` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b373417",
   "metadata": {
    "id": "5b373417",
    "nbgrader": {
     "grade": false,
     "grade_id": "keras",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Please run this cell to install keras\n",
    "#!pip install keras\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db92f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95db92f6",
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-sol",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "6f029d7e-21c1-4b9a-a365-7d4068e429bb"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df_sub4=df.loc[:, ['Page total likes', 'Type', 'Category', 'Post Month', 'Post Weekday', 'Post Hour', 'Paid', 'Total Interactions']]\n",
    "#print(set(df_sub4.columns))\n",
    "#print(df_sub4)\n",
    "\n",
    "colnames=['Type', 'Category', 'Post Month', 'Post Weekday', 'Post Hour']\n",
    "df_OHE = pd.get_dummies(df_sub4, columns=colnames, drop_first=False)\n",
    "print(len(df_OHE.columns))\n",
    "#print(df_OHE)\n",
    "print(len(df_OHE))\n",
    "\n",
    "'''\n",
    "meanval=df_OHE['Page total likes'].mean()\n",
    "stdval=df_OHE['Page total likes'].std()\n",
    "df_OHE['Page total likes']=(df_OHE['Page total likes']-meanval)/stdval\n",
    "\n",
    "meanval=df_OHE['Total Interactions'].mean()\n",
    "stdval=df_OHE['Total Interactions'].std()\n",
    "df_OHE['Total Interactions']=(df_OHE['Total Interactions']-meanval)/stdval\n",
    "'''\n",
    "\n",
    "#df_OHE[['Page total likes','Total Interactions']] = MinMaxScaler().fit_transform(df[['Page total likes','Total Interactions']])\n",
    "\n",
    "df_OHE['Page total likes'] = minmax_scale(df_OHE['Page total likes'])\n",
    "df_OHE['Total Interactions'] = minmax_scale(df_OHE['Total Interactions'])\n",
    "\n",
    "print('Sum')\n",
    "print(math.ceil(df_OHE['Page total likes'].sum()))\n",
    "print(math.ceil(df_OHE['Total Interactions'].sum()))\n",
    "print(math.ceil(df_OHE['Page total likes'].mean()))\n",
    "print(math.ceil(df_OHE['Total Interactions'].mean()))\n",
    "\n",
    "#print(df_OHE['Total Interactions'])\n",
    "\n",
    "X=df_OHE.drop(columns=['Total Interactions'])\n",
    "y=df_OHE['Total Interactions']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(len(X_train))\n",
    "print(len(y_test))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import metrics\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(12, input_dim=50, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[metrics.MeanSquaredError()])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)\n",
    "\n",
    "_, mse = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Neural Net 3-layer accuracy: \"+str(mse))\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97ab82",
   "metadata": {
    "id": "1a97ab82",
    "nbgrader": {
     "grade": true,
     "grade_id": "q2-test1",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Cell-5 -- Do not modify this cell\n",
    "assert (math.ceil(df_OHE['Page total likes'].sum())) == 357\n",
    "assert (math.ceil(df_OHE['Total Interactions'].sum())) == 17\n",
    "assert (math.ceil(df_OHE['Page total likes'].mean())) == 1\n",
    "assert (math.ceil(df_OHE['Total Interactions'].mean())) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba034fe",
   "metadata": {
    "id": "7ba034fe",
    "nbgrader": {
     "grade": true,
     "grade_id": "q2-test2",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Cell-6 -- Do not modify this cell\n",
    "assert len(X_train)==346\n",
    "assert len(y_test)==149\n",
    "assert mse<0.005\n",
    "assert len(model.weights)==6"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
