{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CS3hWIuJEe9-"
   },
   "source": [
    "# Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "TkXwl0p1G6Gl",
    "outputId": "4176369a-1df4-428b-855d-cd758dedf8b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/minor/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/minor/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/minor/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####PLEASE EXECUTE THESE COMMANDS BEFORE PROCEEDING####\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "L_pNvP0lDAZR",
    "outputId": "c27e5da9-707b-4506-f299-66cd78ebb54e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Hello everyone.',\n 'Welcome to Intro to Machine Learning Applications.',\n 'We are now learning important basics of NLP.']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization -- Text into word tokens; Paragraphs into sentences;\n",
    "from nltk.tokenize import sent_tokenize \n",
    "  \n",
    "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqkXKzNOG_CP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Wie geht es Ihnen?', 'Mir geht es gut.']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data \n",
    "  \n",
    "german_tokenizer = nltk.data.load('tokenizers/punkt/PY3/german.pickle') \n",
    "  \n",
    "text = 'Wie geht es Ihnen? Mir geht es gut.'\n",
    "german_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMAa2dymH7f_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Hello',\n 'everyone',\n '.',\n 'Welcome',\n 'to',\n 'Intro',\n 'to',\n 'Machine',\n 'Learning',\n 'Applications',\n '.',\n 'We',\n 'are',\n 'now',\n 'learning',\n 'important',\n 'basics',\n 'of',\n 'NLP',\n '.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdYqZcYCINaK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Hello',\n 'everyone.',\n 'Welcome',\n 'to',\n 'Intro',\n 'to',\n 'Machine',\n 'Learning',\n 'Applications.',\n 'We',\n 'are',\n 'now',\n 'learning',\n 'important',\n 'basics',\n 'of',\n 'NLP',\n '.']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "  \n",
    "tokenizer = TreebankWordTokenizer() \n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_3jEcxcVbA-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['hello everyone',\n 'everyone welcome',\n 'welcome to',\n 'to intro',\n 'intro to',\n 'to machine',\n 'machine learning',\n 'learning applications',\n 'applications we',\n 'we are',\n 'are now',\n 'now learning',\n 'learning important',\n 'important basics',\n 'basics of',\n 'of nlp']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-grams using pure Python\n",
    "\n",
    "import re\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    # Convert to lowercases\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
    "print(text)\n",
    "generate_ngrams(text, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wz-Mq1T6YQSW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'everyone', 'welcome'), ('everyone', 'welcome', 'to'), ('welcome', 'to', 'intro'), ('to', 'intro', 'to'), ('intro', 'to', 'machine'), ('to', 'machine', 'learning'), ('machine', 'learning', 'applications'), ('learning', 'applications', 'we'), ('applications', 'we', 'are'), ('we', 'are', 'now'), ('are', 'now', 'learning'), ('now', 'learning', 'important'), ('learning', 'important', 'basics'), ('important', 'basics', 'of'), ('basics', 'of', 'nlp')]\n"
     ]
    }
   ],
   "source": [
    "# n-grams using NLTK\n",
    "\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "text = text.lower()\n",
    "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 3))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9BG909xTFbeZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone. welcome to intro to machine learning applications. we are now learning important basics of nlp.\n",
      "HELLO EVERYONE. WELCOME TO INTRO TO MACHINE LEARNING APPLICATIONS. WE ARE NOW LEARNING IMPORTANT BASICS OF NLP.\n"
     ]
    }
   ],
   "source": [
    "#Text Normalization\n",
    "\n",
    "# Lowercasing\n",
    "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
    "lowert = text.lower()\n",
    "uppert = text.upper()\n",
    "\n",
    "print(lowert)\n",
    "print(uppert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JxdoZyaY-iP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hike  :  hike\n",
      "hikes  :  hike\n",
      "hiked  :  hike\n",
      "hiking  :  hike\n",
      "hikers  :  hiker\n",
      "hiker  :  hiker\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"hike\", \"hikes\", \"hiked\", \"hiking\", \"hikers\", \"hiker\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "x6EM6ADdZYbL",
    "outputId": "cc7bc613-7f67-4e02-bbeb-14fb7c5d069c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\n",
      "hello everyon welcom to intro to machin learn applic we are now learn import basic of nlp\n"
     ]
    }
   ],
   "source": [
    "# Porter stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "   \n",
    "ps = PorterStemmer() \n",
    "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
    "print(text)\n",
    "\n",
    "#Tokenize and stem the words\n",
    "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "\n",
    "i=0\n",
    "while i<len(tokens):\n",
    "  tokens[i]=ps.stem(tokens[i])\n",
    "  i=i+1\n",
    "\n",
    "#merge all the tokens to form a long text sequence \n",
    "text2 = ' '.join(tokens) \n",
    "\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PQg-2u17aWQh",
    "outputId": "00d2b375-c2cf-4b11-ad8f-4bb9043be5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\n",
      "hello everyon welcom to intro to machin learn applic we are now learn import basic of nlp\n"
     ]
    }
   ],
   "source": [
    "# Snowball stemming\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "   \n",
    "ss = SnowballStemmer(\"english\")\n",
    "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
    "print(text)\n",
    "\n",
    "\n",
    "#Tokenize and stem the words\n",
    "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "\n",
    "i=0\n",
    "while i<len(tokens):\n",
    "  tokens[i]=ss.stem(tokens[i])\n",
    "  i=i+1\n",
    "\n",
    "#merge all the tokens to form a long text sequence \n",
    "text2 = ' '.join(tokens) \n",
    "\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQkySHTBldBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'Welcome', 'to', 'Intro', 'to', 'Machine', 'Learning', 'Applications', '.', 'We', 'are', 'now', 'learning', 'important', 'basics', 'of', 'NLP', '.']\n",
      "['Hello', 'everyone', '.', 'Welcome', 'Intro', 'Machine', 'Learning', 'Applications', '.', 'We', 'learning', 'important', 'basics', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "text = \"Hello everyone. Welcome to Intro to Machine Learning Applications. We are now learning important basics of NLP.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(text) \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "print(word_tokens) \n",
    "print(filtered_sentence) \n",
    "\n",
    "text2 = ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ejWwVdZebHlA",
    "outputId": "a90010e5-7deb-4f1e-9ea1-03b5e0f0da44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('GitHub', 'NNP'),\n ('is', 'VBZ'),\n ('a', 'DT'),\n ('development', 'NN'),\n ('platform', 'NN'),\n ('inspired', 'VBN'),\n ('by', 'IN'),\n ('the', 'DT'),\n ('way', 'NN'),\n ('you', 'PRP'),\n ('work', 'VBP'),\n ('.', '.'),\n ('From', 'IN'),\n ('open', 'JJ'),\n ('source', 'NN'),\n ('to', 'TO'),\n ('business', 'NN'),\n (',', ','),\n ('you', 'PRP'),\n ('can', 'MD'),\n ('host', 'VB'),\n ('and', 'CC'),\n ('review', 'VB'),\n ('code', 'NN'),\n (',', ','),\n ('manage', 'NN'),\n ('projects', 'NNS'),\n (',', ','),\n ('and', 'CC'),\n ('build', 'VB'),\n ('software', 'NN'),\n ('alongside', 'RB'),\n ('40', 'CD'),\n ('million', 'CD'),\n ('developers', 'NNS'),\n ('.', '.')]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part-of-Speech tagging\n",
    "import nltk\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    sentence = nltk.pos_tag(sentence)\n",
    "    return sentence\n",
    "\n",
    "preprocess('GitHub is a development platform inspired by the way you work. From open source to business, you can host and review code, manage projects, and build software alongside 40 million developers.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
